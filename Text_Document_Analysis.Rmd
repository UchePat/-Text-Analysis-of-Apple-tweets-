---
title: "Text Document Analysis"
author: "Uche Kalu"
date: "4/17/2022"
output:
  pdf_document: default
  html_document: default
---

**Text Analysis** **is a Natural Language Processing (NLP) technique that helps businesses monitor brand and product sentiment in customer feedback**

### DATA

```{r save, message=FALSE, warning=FALSE}
mydata <- read.table("apple.txt", header = T, sep = ',')
```

### Build Corpus. 

**Corpus is a collection of documents so each tweet will be treated as a document**

```{r results='hide', message=FALSE, warning=FALSE}
library(tm)        

mycorpus <- iconv(mydata$text, to = "UTF-8")   

mycorpus <- Corpus(VectorSource(mycorpus))

```

### Cleaning the Tweets/Text corpuses

**Here we change all text to lower-case, remove punctuation marks, removes all stop-words, remove URL links**

```{r results='hide', message=FALSE, warning=FALSE}
mycorpus <- tm_map(mycorpus, tolower)  

mycorpus <- tm_map(mycorpus, removePunctuation)  

mycorpus <- tm_map(mycorpus, removeNumbers)

ourcorpus <- tm_map(mycorpus, removeWords, stopwords('english'))   

removeURL <- function(x) gsub('http[[:alnum:]]*', '', x)   
ourcorpus <- tm_map(ourcorpus, content_transformer(removeURL))  


ourcorpus <- tm_map(ourcorpus, removeWords, c('aapl', 'apple'))   

ourcorpus <- tm_map(ourcorpus, gsub, 
                   pattern = 'stocks', 
                   replacement = 'stock')   

ourcorpus <- tm_map(ourcorpus, stripWhitespace)  
```

### Term Document Matrix

**Here we convert the unstructured data into a matrix (rows and columns)**

```{r results='hide', message=FALSE, warning=FALSE}
tdm <- TermDocumentMatrix(ourcorpus)
tdm      
```

```{r}
tdm <- as.matrix(tdm)
tdm[1:10, 1:20]
```

**Lets find out how often a word appears in the dataset**

```{r}
w <- rowSums(tdm)    
View(w)
```

**Lets only look at words that appear 25 times or more in the dataset**

```{r}
w <- subset(w, w >= 25)   
View(w)
```

### Visualizing the words

**Using a Bar plot to view the frequency count of each word**

```{r, fig.width=10}
barplot(w, las = 2, col = rainbow(50))
```

#### Creating a Word-Cloud

```{r results='hide', message=FALSE, warning=FALSE}
library(wordcloud)

w <- sort(rowSums(tdm), decreasing = TRUE)  

set.seed(222)
```

```{r message=FALSE, warning=FALSE}
wordcloud(words = names(w),    
          freq = w)
```

```{r}
wordcloud(words = names(w),
          freq = w,
          max.words = 200,
          random.order = F,
          min.freq = 5,                    
          colors = brewer.pal(8, 'Dark2'),  
          scale = c(5, 0.3),               
          rot.per = 0.5)  
```

**Displaying the Word-Cloud in different shape format**

```{r}
library(wordcloud2)

w <- data.frame(names(w), w)

colnames(w) <- c('word', 'freq')  
head(w)
```

```{r}
wordcloud2(w,
           size = 0.7,
           shape = 'triangle',    
           rotateRatio = 0.5,
           minSize = 1)
```

```{r}
wordcloud2(w,
           size = 0.5,
           shape = 'circle',    
           rotateRatio = 0.5,
           minSize = 1)
```

### 
